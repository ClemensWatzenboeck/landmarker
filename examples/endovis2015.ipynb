{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <a href=\"https://predict-idlab.github.io/landmarker\">\n",
    "        <img alt=\"landmarker\" src=\"https://raw.githubusercontent.com/predict-idlab/landmarker/main/docs/_static/images/logo.svg\" width=\"66%\">\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# Training and Evaluating Static Heatmap Regression Model for Multi-Instance and Multi-Class Landmark Detetection (EndoVis 2015 Challenge)\n",
    "\n",
    "In this tutorial, we will train and evaluate an direct static heatmap regression model for landmark \n",
    "detection with EndoVis 2015 Challenge. We will use part of the EndoVis 2015 challenge dataset to \n",
    "construct a multi-instance and multi-class landmark detection task. The dataset contains 4 training \n",
    "and 6 testing videos of robotic surgery. The goal is to predict the location of instruments in the video, \n",
    "more specifically the tip of the clasper. We only consider the clasper points and ignore the other points,\n",
    "since they are way more ambiguous. One of the difficulties \n",
    "\n",
    "The videos are transformed into images and the annotations are \n",
    "given as 2D points. The dataset is split into a training and testing set. The training set contains 4 videos and \n",
    "the testing set contains 6 videos, such as specified in the challenge. \n",
    "\n",
    "We will go through the following steps:\n",
    "* [Loading the dataset](#Loading-the-dataset)\n",
    "* [Inspecting the dataset](#Inspecting-the-dataset)\n",
    "* [Training and initializing the UNet model](#Training-the-model)\n",
    "* [Evaluating the model](#Evaluating-the-model)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/predict-idlab/landmarker/examples/static_unet_endovis2015.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"import landmarker\" || pip install landmarker\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.transforms import (Compose, RandAffined, RandGaussianNoised, ScaleIntensityd,\n",
    "                              RandScaleIntensityd, RandAdjustContrastd, RandHistogramShiftd)\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_keys = ('image', 'mask')\n",
    "spatial_transformd = [RandAffined(fn_keys, prob=1,\n",
    "                        rotate_range=(-np.pi/12, np.pi/12),\n",
    "                        translate_range=(-10, 10),\n",
    "                        scale_range=(-0.1, 0.1),\n",
    "                        shear_range=(-0.1, 0.1)\n",
    "                        )]\n",
    "\n",
    "train_transformd = Compose([\n",
    "                            RandGaussianNoised(('image', ), prob=0.2, mean=0, std=0.1),  # Add gaussian noise\n",
    "                            RandScaleIntensityd(('image', ), factors=0.25, prob=0.2),  # Add random intensity scaling\n",
    "                            RandAdjustContrastd(('image', ), prob=0.2, gamma=(0.5,4.5)),  # Randomly adjust contrast\n",
    "                            RandHistogramShiftd(('image', ), prob=0.2),  # Randomly shift histogram\n",
    "                            ScaleIntensityd(('image', )),  # Scale intensity\n",
    "                        ] + spatial_transformd)\n",
    "\n",
    "inference_transformd = Compose([\n",
    "    ScaleIntensityd(('image', )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.datasets import get_endovis2015_heatmap_datasets\n",
    "\n",
    "data_dir = \"/Users/jefjonkers/Data/landmark-datasets\"\n",
    "ds_train, ds_test = get_endovis2015_heatmap_datasets(data_dir, train_transform = train_transformd,\n",
    "                                                     inference_transform= inference_transformd,\n",
    "                                                     dim_img = (512, 512), sigma=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.visualize import inspection_plot\n",
    "\n",
    "# Plot the first 3 images from the training set\n",
    "inspection_plot(ds_train, np.random.randint(0, len(ds_train), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 images from the test1 set\n",
    "inspection_plot(ds_test, range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and initializing the SpatialConfiguration model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
    "                                                          patience=20, verbose=True, cooldown=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the data loaders and split training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lengths = [0.8, 0.2]\n",
    "ds_train_train, ds_train_val = torch.utils.data.random_split(ds_train, split_lengths)\n",
    "train_loader = DataLoader(ds_train_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(ds_train_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.heatmap.decoder import heatmap_to_coord, heatmap_to_multiple_coord\n",
    "from landmarker.metrics import point_error\n",
    "\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "from landmarker.metrics.metrics import multi_instance_point_error\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        heatmaps = batch[\"mask\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, heatmaps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def val_epoch(model, val_loader, criterion, device):\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(tqdm(val_loader)):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            heatmaps = batch[\"mask\"].to(device)\n",
    "            landmarks = batch[\"landmark\"].to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, heatmaps)\n",
    "            eval_loss += loss.item()\n",
    "    return eval_loss / len(val_loader)\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs=1000):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        ds_train.transform = None\n",
    "        val_loss = val_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f}\")\n",
    "        lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, device,\n",
    "      epochs=epochs)\n",
    "# model.load_state_dict(torch.load(\"best_weights_unet_endovis_static.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_landmarks = []\n",
    "true_landmarks = []\n",
    "dim_origs = []\n",
    "pixel_spacings = []\n",
    "paddings = []\n",
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "test_mpe = 0\n",
    "test_tp = 0\n",
    "test_fp = 0\n",
    "test_fn = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        images = batch[\"image\"]\n",
    "        heatmaps = batch[\"mask\"]\n",
    "        landmarks = batch[\"landmark\"]\n",
    "        outputs = model(images.to(device)).detach().cpu()\n",
    "        offset_coords = outputs.shape[1]-landmarks.shape[1]\n",
    "        pred_landmarks_list, _ = heatmap_to_multiple_coord(sigmoid(outputs), window=5,\n",
    "                                                           threshold=0.5,\n",
    "                                                           method=\"argmax\")\n",
    "        pe_batch, tp_batch, fp_batch, fn_batch, pred_landmarks_torch = multi_instance_point_error(\n",
    "            true_landmarks=landmarks, pred_landmarks=pred_landmarks_list, dim=(512, 512),\n",
    "            dim_orig=batch[\"dim_original\"], pixel_spacing=batch[\"spacing\"],\n",
    "            padding=batch[\"padding\"], reduction=\"none\")\n",
    "        test_mpe += torch.nanmean(pe_batch).item()\n",
    "        test_tp += torch.nansum(tp_batch).item()\n",
    "        test_fp += torch.nansum(fp_batch).item()\n",
    "        test_fn += torch.nansum(fn_batch).item()\n",
    "        pred_landmarks.append(pred_landmarks_torch)\n",
    "        true_landmarks.append(landmarks)\n",
    "        dim_origs.append(batch[\"dim_original\"])\n",
    "        pixel_spacings.append(batch[\"spacing\"])\n",
    "        paddings.append(batch[\"padding\"])\n",
    "        tp.append(tp_batch)\n",
    "        fp.append(fp_batch)\n",
    "        fn.append(fn_batch)\n",
    "\n",
    "\n",
    "test_mpe /= len(test_loader)\n",
    "\n",
    "print(f\"Test Mean PE: {test_mpe:.4f}\")\n",
    "print(f\"Test TP: {test_tp:.4f}\")\n",
    "print(f\"Test FP: {test_fp:.4f}\")\n",
    "print(f\"Test FN: {test_fn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.metrics import sdr\n",
    "\n",
    "sdr_test = sdr([4, 5, 10, 20], true_landmarks=torch.cat(true_landmarks, axis=0), pred_landmarks=torch.cat(pred_landmarks, axis=0),\n",
    "               dim=(512, 512), dim_orig=torch.cat(dim_origs, axis=0).int(), pixel_spacing=torch.cat(pixel_spacings, axis=0),\n",
    "               padding=torch.cat(paddings, axis=0))\n",
    "for key in sdr_test:\n",
    "    print(f\"SDR for {key}mm: {sdr_test[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.visualize.utils import prediction_inspect_plot_multi_instance\n",
    "\n",
    "model.to(\"cpu\")\n",
    "prediction_inspect_plot_multi_instance(ds_test, model, range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.visualize import plot_cpe\n",
    "\n",
    "plot_cpe(torch.cat(true_landmarks, axis=0), torch.cat(pred_landmarks, axis=0), dim=(512, 512),\n",
    "            dim_orig=torch.cat(dim_origs, axis=0).int(), pixel_spacing=torch.cat(pixel_spacings, axis=0),\n",
    "            padding=torch.cat(paddings, axis=0), class_names=ds_test.class_names,\n",
    "            group=False, title=\"CPE curve\", save_path=None,\n",
    "            stat='proportion', unit='pixels', kind='ecdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmarker.visualize.evaluation import multi_instance_detection_report\n",
    "\n",
    "multi_instance_detection_report(torch.cat(true_landmarks, axis=0), torch.cat(pred_landmarks, axis=0),\n",
    "                                torch.cat(tp, axis=0), torch.cat(fp, axis=0), torch.cat(fn, axis=0), dim=(512, 512),\n",
    "                                dim_orig=torch.cat(dim_origs, axis=0).int(), pixel_spacing=torch.cat(pixel_spacings, axis=0),\n",
    "                                padding=torch.cat(paddings, axis=0), class_names=ds_test.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
